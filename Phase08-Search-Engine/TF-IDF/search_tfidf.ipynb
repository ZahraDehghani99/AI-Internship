{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyEhuis4mqN5"
      },
      "source": [
        "# Build semantic document search engine with TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNRbTnakmqOA"
      },
      "source": [
        "In this notebook, we are using some of (7 of them) hamshahri newspapare dataset for our work.\n",
        "At the beginning, we shoud store them in mongodb collection whose name in 'ham_2007'. we shoud preprocess our texts of news in order to find unique words and create tf-idf model with them.\n",
        "\n",
        "**I run this notebook in vscode.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FkRdGyY-mqOF"
      },
      "outputs": [],
      "source": [
        "from parsivar import Normalizer, Tokenizer, FindStems\n",
        "\n",
        "# plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "# connet to mongodb\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Library for parsing XML\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DddUGLf-mqOM",
        "outputId": "87e61e69-4650-4cd5-9032-c777378e8c9d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading NLTk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create MongoDB collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# connect to mongodb client\n",
        "client = MongoClient('localhost', port=27017)\n",
        "db = client.search_engine # create a database whose name is 'search_engine'\n",
        "ham_2007 = db.ham_2007 # create a collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XML files in path : ['HAM2-070101.xml', 'HAM2-070102.xml', 'HAM2-070103.xml', 'HAM2-070104.xml', 'HAM2-070106.xml', 'HAM2-070107.xml', 'HAM2-070110.xml']\n"
          ]
        }
      ],
      "source": [
        "#Store file names in the filename\n",
        "filename = os.listdir(u\"/Users/User/Flask_Basics/Search_Engine/TF-IDF/dataset\")\n",
        "print(f'XML files in path : {filename}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filepath = \"/Users/User/Flask_Basics/Search_Engine/TF-IDF/dataset\"\n",
        "\n",
        "\n",
        "def xml_to_mongodb(filepath, filename):\n",
        "    # parse xml file\n",
        "    for i in range(len(filename)):\n",
        "        tree = ET.parse(os.path.join(filepath, filename[i]))\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # write xml file on database \n",
        "        for elem in root.findall(\"DOC\"):\n",
        "            rows = []\n",
        "\n",
        "            # parse DOCID\n",
        "            doc_id = elem.find(\"DOCID\")\n",
        "            if doc_id != None:\n",
        "                doc_id = doc_id.text\n",
        "            rows.append(doc_id)\n",
        "\n",
        "            # parse TITLE\n",
        "            title = elem.find(\"TITLE\")\n",
        "            if title != None:\n",
        "                title = re.sub('\\n', ' ', title.text)\n",
        "            rows.append(title)  \n",
        "\n",
        "            # parse TEXT\n",
        "            sent = elem.find(\"TEXT\")\n",
        "            if sent != None:\n",
        "                sentence = list(sent)[-1].tail.strip() if list(sent) else sent.text.strip()\n",
        "                sentence = re.sub('\\n', ' ', sentence)\n",
        "            rows.append(sentence)\n",
        "\n",
        "            # save id, title and text of a news in json file\n",
        "            ham_2007.insert_one({\"doc_id\":rows[0] , \"doc_title\":rows[1], \"doc_text\":rows[2]})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# store xml files in monogdb collection whose name is 'ham_2007'\n",
        "xml_to_mongodb(filepath, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gORB7vzwmqOl"
      },
      "source": [
        "## Create dataframe from monogdb data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sJN_MdXRmqOn"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DSFTgFW4mqOo"
      },
      "outputs": [],
      "source": [
        "from pymongo import MongoClient\n",
        "\n",
        "# connect to mongodb client to accest to created database\n",
        "client = MongoClient('localhost', port=27017)\n",
        "db = client.search_engine # switch to database whose name is 'search_engine'\n",
        "ham_2007= db.ham_2007 # switch to a collection whose name is 'ham_2007'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "peePjiW5mqOp"
      },
      "outputs": [],
      "source": [
        "title = []\n",
        "text = []\n",
        "\n",
        "for record in ham_2007.find():\n",
        "    \n",
        "    title.append(record[\"doc_title\"])\n",
        "    text.append(record[\"doc_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "I0VoLdTfmqOt",
        "outputId": "afcb6585-4cf8-4c49-c0a1-83a8c2448784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of title : 731\n",
            "length of text : 731\n"
          ]
        }
      ],
      "source": [
        "print(f'length of title : {len(title)}')\n",
        "print(f'length of text : {len(text)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "edSLcmejmqOv"
      },
      "outputs": [],
      "source": [
        "df['title'] = title\n",
        "df['text'] = text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ekRaUoOcmqOw",
        "outputId": "ae568de5-ef16-4464-d69d-b78393c96c3e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>مديركل كتاب و كتابخواني وزارت فرهنگ و ارشاد ا...</td>\n",
              "      <td>فارس: مدير كل كتاب  و كتاب خواني وزارت فرهنگ و...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>مرگ هري پاتر و دردسر پستي انگلستان</td>\n",
              "      <td>ايسنا: در حالي كه تاريخ دقيق انتشار آخرين كتاب...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>تغيير در اجراي اسكار 2007</td>\n",
              "      <td>ايسنا: نامزدهاي بهترين فيلم خارجي مراسم اسكار ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>اخبار كوتاه</td>\n",
              "      <td>چاپ 24 شازده كوچولو: ترجمه شازده كوچولوي محمد ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>مهران مديري: پا در كفش بزرگان كرده ام</td>\n",
              "      <td>فارس: مهران مديري گفت: ترانه هايي را كه در مجم...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  \\\n",
              "0   مديركل كتاب و كتابخواني وزارت فرهنگ و ارشاد ا...   \n",
              "1                مرگ هري پاتر و دردسر پستي انگلستان    \n",
              "2                         تغيير در اجراي اسكار 2007    \n",
              "3                                       اخبار كوتاه    \n",
              "4             مهران مديري: پا در كفش بزرگان كرده ام    \n",
              "\n",
              "                                                text  \n",
              "0  فارس: مدير كل كتاب  و كتاب خواني وزارت فرهنگ و...  \n",
              "1  ايسنا: در حالي كه تاريخ دقيق انتشار آخرين كتاب...  \n",
              "2  ايسنا: نامزدهاي بهترين فيلم خارجي مراسم اسكار ...  \n",
              "3  چاپ 24 شازده كوچولو: ترجمه شازده كوچولوي محمد ...  \n",
              "4  فارس: مهران مديري گفت: ترانه هايي را كه در مجم...  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "afSeybFkmqOy",
        "outputId": "8b1641c5-64f6-486f-f8ba-7d340be06cab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(731, 2)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's save this dataframe in order to use it in google colab for applying preprocessing function on the 'text' column with pandarallel library. (For using pandarallel in windows, we should install wsl (windows subsystem linux) on it.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # save dataframe\n",
        "# df.to_pickle(\"/Users/User/Flask_Basics/Search_Engine/TF-IDF/df\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKZKK8IrmqOQ"
      },
      "source": [
        "## Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V0qhTkcmqOS"
      },
      "source": [
        "### Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "xAeKxIJNmqOV"
      },
      "outputs": [],
      "source": [
        "# define a function to read file\n",
        "def readFile(filename):\n",
        "  fileObj = open(filename, 'r', encoding ='utf-8') # open the file in read mode\n",
        "  words = fileObj.read().splitlines() # puts the file into an array\n",
        "  fileObj.close()\n",
        "  return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Q8ARD_5kmqOW",
        "outputId": "04709ced-064e-4343-cfec-9b22a98a0876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of stop words : 1370\n"
          ]
        }
      ],
      "source": [
        "stopwords = readFile('/Users/User/Flask_Basics/persian_stopwords_kharazi.txt')\n",
        "print(f'length of stop words : {len(stopwords)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Zkam-_xAmqOb",
        "outputId": "52680fd2-a263-4534-8965-faaa8a99a74f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "some of stop words : ['!', '\"', '#', '(', ')', '*', ',', '-', '.', '/']\n"
          ]
        }
      ],
      "source": [
        "print(f'some of stop words : {stopwords[:10]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k63qbiT4mqOg"
      },
      "source": [
        "### Preprocessing Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyQ3gjZAmqOh"
      },
      "source": [
        "Because, after stemming we have '&' in between of our verbs, we should delete them in function delete_and."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "kfZqX8zAmqOj"
      },
      "outputs": [],
      "source": [
        "def delete_and(word):\n",
        "  idx = word.find(\"&\")\n",
        "  if idx!=-1:\n",
        "    word = word[:idx]\n",
        "  return word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "tKSLhbpDmqOk"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing(review, stopwords):\n",
        "  try:\n",
        "    # replace half-space with ' '\n",
        "    review =  re.sub('\\u200c', ' ',review)\n",
        "\n",
        "    # Normalizing the text\n",
        "    # First we should normalize text in order to convert persian numbers into english numbers then\n",
        "    # with following function (filter) delete them\n",
        "    # Because, some comments are pinglish, we should set pinglish_conversion_needed = True\n",
        "    normalizer = Normalizer() \n",
        "    review = normalizer.normalize(review)\n",
        "\n",
        "    # because after normalization appear some '/u200c', we should replace them with space\n",
        "    review =  re.sub('\\u200c', ' ',review)\n",
        "\n",
        "    # delete english characters and numbers from sentences\n",
        "    review = filter(lambda x: x in string.whitespace or x not in string.printable, review)\n",
        "    review = ''.join(ch for ch in list(review))\n",
        "    \n",
        "    if review != ' ':\n",
        "      # word tokenization\n",
        "      tokenizer = Tokenizer()\n",
        "      words = tokenizer.tokenize_words(review)\n",
        "\n",
        "      # stemming \n",
        "      stemmer = FindStems()\n",
        "      review = [stemmer.convert_to_stem(word) for word in words]\n",
        "\n",
        "      # we should delete '&', because after stemming we have '&' in between of our verbs\n",
        "      review = [delete_and(word) for word in review]\n",
        "\n",
        "      # remove stop words\n",
        "      words_without_stopword = filter(lambda x: x not in stopwords, review)\n",
        "      words_without_stopwords = list(words_without_stopword)\n",
        "    \n",
        "      # join words in preprocessed review\n",
        "      review = ' '.join(words_without_stopwords)\n",
        "    \n",
        "    return review\n",
        "\n",
        "  except TypeError:\n",
        "    print(review)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "stemmer = FindStems()\n",
        "rev = stemmer.convert_to_stem('است')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'اس'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlqRJ_hqmqOz"
      },
      "source": [
        "### Apply preprocessing function on the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This work took 38 seconds for me."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-Cpp5SuWmqO0"
      },
      "outputs": [],
      "source": [
        "df['clean_text'] = df['text'].apply(lambda text: data_preprocessing(text, stopwords))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>مديركل كتاب و كتابخواني وزارت فرهنگ و ارشاد ا...</td>\n",
              "      <td>فارس: مدير كل كتاب  و كتاب خواني وزارت فرهنگ و...</td>\n",
              "      <td>فارس مدیر کتاب کتاب خواند وزارت فرهنگ ارشاد اس...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>مرگ هري پاتر و دردسر پستي انگلستان</td>\n",
              "      <td>ايسنا: در حالي كه تاريخ دقيق انتشار آخرين كتاب...</td>\n",
              "      <td>ایسنا تاریخ دقیق انتشار آخرین کتاب مجموعه داست...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>تغيير در اجراي اسكار 2007</td>\n",
              "      <td>ايسنا: نامزدهاي بهترين فيلم خارجي مراسم اسكار ...</td>\n",
              "      <td>ایسنا نامزد فیلم خارجی مراسم اسکار مرحله معرفی...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>اخبار كوتاه</td>\n",
              "      <td>چاپ 24 شازده كوچولو: ترجمه شازده كوچولوي محمد ...</td>\n",
              "      <td>چاپ شازده کوچولو ترجمه شازده کوچولو محمد قاضی ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>مهران مديري: پا در كفش بزرگان كرده ام</td>\n",
              "      <td>فارس: مهران مديري گفت: ترانه هايي را كه در مجم...</td>\n",
              "      <td>فارس مهران مدیر ترانه مجموعه باغ مظفر خوانده ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>تصحيح و توضيح خبر فولادوند</td>\n",
              "      <td>روز شنبه در صفحه يك روزنامه، خبري با عنوان عزت...</td>\n",
              "      <td>شنبه صفحه روزنامه خبری عزت الله فولادوند دوران...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>سايه روشن</td>\n",
              "      <td>مراسم روز هفت عبداللهي: مراسم گراميداشت هفتمين...</td>\n",
              "      <td>مراسم عبداللهی مراسم گرامیداشت هفتمین درگذشت ن...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>برپايي نمايشگاه عكس سماع عاشقانوآرامگاه مولانا</td>\n",
              "      <td>گروه ادب و هنر براي اولين بار در كشور توسط يك ...</td>\n",
              "      <td>گروه ادب هنر کشور عکاس ایرانی مجموعه عکس سماع ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>عكس روز</td>\n",
              "      <td>عكس: رويترز‎/ ابوهيكل يك فروشگاه در فلسطين، عر...</td>\n",
              "      <td>عکس رویترز ابوهیکل فروشگاه فلسطین عروسک بن لاد...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>در شهر چه خبر</td>\n",
              "      <td>نمايشگر هاي همشهري رسانه اي با 300 هزار بيننده...</td>\n",
              "      <td>نمایشگر همشهری رسانه بیننده نمایشگر خبری همشهر...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>بازار</td>\n",
              "      <td>ركود در بازار برنج بازار برنج به علت نبود تقاض...</td>\n",
              "      <td>رکود بازار برنج بازار برنج علت تقاضا راکدشده ق...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>بازتاب</td>\n",
              "      <td>گروه ادب و هنر- انتشار اولين شماره كتاب همشهري...</td>\n",
              "      <td>گروه ادب هنر انتشار شماره کتاب همشهری استقبال ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>ابتكار ستودني همشهري دكتر غلامعلي حداد عادل</td>\n",
              "      <td>به تجربه ثابت شده كه اختراع انواع رسانه ها در ...</td>\n",
              "      <td>تجربه ثابت اختراع انواع رسانه عصر جایگاه کتاب ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>ارزش سهام 10 شركت بورس از يك ميليارد دلار فرا...</td>\n",
              "      <td>گروه بورس- تازه ترين بررسي هاي انجام شده نشان ...</td>\n",
              "      <td>گروه بورس بررسی ارزش سهام شرکت بورس تهران دلار...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>اعلام برنامه واگذاري 60 ميليارد دلار سهام شرك...</td>\n",
              "      <td>مهر: مقامات وزارت نفت اعلام كردند ارزش سهام 12...</td>\n",
              "      <td>مهر مقامات وزارت نفت ارزش سهام شرکت نفتی دولتی...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>روندها بانك هاي خصوصي در جستجوي شرايط مناسب</td>\n",
              "      <td>گروه بورس - نظام پولي و سيستم بانكي كشور نقش ب...</td>\n",
              "      <td>گروه بورس نظام پولی سیستم بانکی کشور نقش سزا ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>نبض بورس در دست سهام ساختمان</td>\n",
              "      <td>گروه بورس 60 درصد حجم معاملات بورس تهران درروز...</td>\n",
              "      <td>گروه بورس درصد حجم معامله بورس تهران درروز شنب...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>اصلاح آيين نامه قيمت گذاري سهام دولتي آغاز شد</td>\n",
              "      <td>مهر: اصلاح آيين نامه قيمت گذاري سهام دولتي در ...</td>\n",
              "      <td>مهر اصلاح آیین نامه قیمت گذاشت سهام دولتی هیات...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>صف خريد پارسيان به 15 ميليون سهم رسيد</td>\n",
              "      <td>گروه بورس در معاملات روز شنبه صف هاي خريد سهام...</td>\n",
              "      <td>گروه بورس معامله شنبه صف خرید سهام بانک پارسیا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>تلويزيون چنان كه بايد، توليد و پخش فيلم مستند...</td>\n",
              "      <td>تهيه گزارش درباره فيلم مستند و مشكلات فراوان د...</td>\n",
              "      <td>تهیه گزارش فیلم مشکلات فراوان مسیر ساخت تولید ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                title  \\\n",
              "0    مديركل كتاب و كتابخواني وزارت فرهنگ و ارشاد ا...   \n",
              "1                 مرگ هري پاتر و دردسر پستي انگلستان    \n",
              "2                          تغيير در اجراي اسكار 2007    \n",
              "3                                        اخبار كوتاه    \n",
              "4              مهران مديري: پا در كفش بزرگان كرده ام    \n",
              "5                         تصحيح و توضيح خبر فولادوند    \n",
              "6                                          سايه روشن    \n",
              "7     برپايي نمايشگاه عكس سماع عاشقانوآرامگاه مولانا    \n",
              "8                                            عكس روز    \n",
              "9                                      در شهر چه خبر    \n",
              "10                                             بازار    \n",
              "11                                            بازتاب    \n",
              "12       ابتكار ستودني همشهري دكتر غلامعلي حداد عادل    \n",
              "13   ارزش سهام 10 شركت بورس از يك ميليارد دلار فرا...   \n",
              "14   اعلام برنامه واگذاري 60 ميليارد دلار سهام شرك...   \n",
              "15       روندها بانك هاي خصوصي در جستجوي شرايط مناسب    \n",
              "16                      نبض بورس در دست سهام ساختمان    \n",
              "17     اصلاح آيين نامه قيمت گذاري سهام دولتي آغاز شد    \n",
              "18             صف خريد پارسيان به 15 ميليون سهم رسيد    \n",
              "19   تلويزيون چنان كه بايد، توليد و پخش فيلم مستند...   \n",
              "\n",
              "                                                 text  \\\n",
              "0   فارس: مدير كل كتاب  و كتاب خواني وزارت فرهنگ و...   \n",
              "1   ايسنا: در حالي كه تاريخ دقيق انتشار آخرين كتاب...   \n",
              "2   ايسنا: نامزدهاي بهترين فيلم خارجي مراسم اسكار ...   \n",
              "3   چاپ 24 شازده كوچولو: ترجمه شازده كوچولوي محمد ...   \n",
              "4   فارس: مهران مديري گفت: ترانه هايي را كه در مجم...   \n",
              "5   روز شنبه در صفحه يك روزنامه، خبري با عنوان عزت...   \n",
              "6   مراسم روز هفت عبداللهي: مراسم گراميداشت هفتمين...   \n",
              "7   گروه ادب و هنر براي اولين بار در كشور توسط يك ...   \n",
              "8   عكس: رويترز‎/ ابوهيكل يك فروشگاه در فلسطين، عر...   \n",
              "9   نمايشگر هاي همشهري رسانه اي با 300 هزار بيننده...   \n",
              "10  ركود در بازار برنج بازار برنج به علت نبود تقاض...   \n",
              "11  گروه ادب و هنر- انتشار اولين شماره كتاب همشهري...   \n",
              "12  به تجربه ثابت شده كه اختراع انواع رسانه ها در ...   \n",
              "13  گروه بورس- تازه ترين بررسي هاي انجام شده نشان ...   \n",
              "14  مهر: مقامات وزارت نفت اعلام كردند ارزش سهام 12...   \n",
              "15  گروه بورس - نظام پولي و سيستم بانكي كشور نقش ب...   \n",
              "16  گروه بورس 60 درصد حجم معاملات بورس تهران درروز...   \n",
              "17  مهر: اصلاح آيين نامه قيمت گذاري سهام دولتي در ...   \n",
              "18  گروه بورس در معاملات روز شنبه صف هاي خريد سهام...   \n",
              "19  تهيه گزارش درباره فيلم مستند و مشكلات فراوان د...   \n",
              "\n",
              "                                           clean_text  \n",
              "0   فارس مدیر کتاب کتاب خواند وزارت فرهنگ ارشاد اس...  \n",
              "1   ایسنا تاریخ دقیق انتشار آخرین کتاب مجموعه داست...  \n",
              "2   ایسنا نامزد فیلم خارجی مراسم اسکار مرحله معرفی...  \n",
              "3   چاپ شازده کوچولو ترجمه شازده کوچولو محمد قاضی ...  \n",
              "4   فارس مهران مدیر ترانه مجموعه باغ مظفر خوانده ا...  \n",
              "5   شنبه صفحه روزنامه خبری عزت الله فولادوند دوران...  \n",
              "6   مراسم عبداللهی مراسم گرامیداشت هفتمین درگذشت ن...  \n",
              "7   گروه ادب هنر کشور عکاس ایرانی مجموعه عکس سماع ...  \n",
              "8   عکس رویترز ابوهیکل فروشگاه فلسطین عروسک بن لاد...  \n",
              "9   نمایشگر همشهری رسانه بیننده نمایشگر خبری همشهر...  \n",
              "10  رکود بازار برنج بازار برنج علت تقاضا راکدشده ق...  \n",
              "11  گروه ادب هنر انتشار شماره کتاب همشهری استقبال ...  \n",
              "12  تجربه ثابت اختراع انواع رسانه عصر جایگاه کتاب ...  \n",
              "13  گروه بورس بررسی ارزش سهام شرکت بورس تهران دلار...  \n",
              "14  مهر مقامات وزارت نفت ارزش سهام شرکت نفتی دولتی...  \n",
              "15  گروه بورس نظام پولی سیستم بانکی کشور نقش سزا ا...  \n",
              "16  گروه بورس درصد حجم معامله بورس تهران درروز شنب...  \n",
              "17  مهر اصلاح آیین نامه قیمت گذاشت سهام دولتی هیات...  \n",
              "18  گروه بورس معامله شنبه صف خرید سهام بانک پارسیا...  \n",
              "19  تهیه گزارش فیلم مشکلات فراوان مسیر ساخت تولید ...  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(731, 3)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DuFN4zzimqO1"
      },
      "outputs": [],
      "source": [
        "# # save dataframe\n",
        "# df.to_pickle(\"/Users/User/Flask_Basics/Search_Engine/TF-IDF/df_clean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "s3oyxlZ4mqO2"
      },
      "outputs": [],
      "source": [
        "#load the preprocessing dataframe\n",
        "df= pd.read_pickle(r'/Users/User/Flask_Basics/Search_Engine/TF-IDF/df_clean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "STkf8ODomqO2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 731 entries, 0 to 730\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   title       731 non-null    object\n",
            " 1   text        731 non-null    object\n",
            " 2   clean_text  731 non-null    object\n",
            "dtypes: object(3)\n",
            "memory usage: 17.3+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvtd2bcfmqO3"
      },
      "source": [
        "Let's save clean textes a in mongodb collectin whose name is 'ham_clean'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TT2CBt9BmqO4"
      },
      "outputs": [],
      "source": [
        "ham_clean = db.ham_clean\n",
        "\n",
        "for i in range(df.shape[0]):\n",
        "    ham_clean.insert_one({\"title\": df[\"title\"][i], \"text\":df[\"clean_text\"][i]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uRE0r9nmqO5"
      },
      "source": [
        "## Document Search engine with TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8uuA_X3mqO6"
      },
      "source": [
        "### Generated TF-IDF by using TfidfVectorizer from Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Z2O40JtOmqO7"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create Vocabulary\n",
        "vocabulary = set()\n",
        "\n",
        "# find unique words in whole of the documents and append them to vocabulary\n",
        "for doc in df.clean_text:\n",
        "    vocabulary.update(doc.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of vocabulary : 12975\n",
            "some of vocabulary : ['کبیسه', 'ارضا', 'زاویه', 'خودویران', 'ناهماهنگ', 'خفگی', 'کونل', 'ماشائالله', 'تیرگی', 'شولم', 'رضاییان', 'موثربا', 'کالری', 'البیت', 'بهروز']\n"
          ]
        }
      ],
      "source": [
        "vocabulary = list(vocabulary)\n",
        "print(f'length of vocabulary : {len(vocabulary)}')\n",
        "print(f'some of vocabulary : {vocabulary[:15]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Intializating the tfIdf model\n",
        "tfidf = TfidfVectorizer(vocabulary=vocabulary)\n",
        "tfidf_tran = tfidf.fit_transform(df['clean_text'])# Fit and Transform the TfIdf model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tfidf_tran shape : (731, 12975)\n"
          ]
        }
      ],
      "source": [
        "print(f'tfidf_tran shape : {tfidf_tran.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type of tfidf : <class 'sklearn.feature_extraction.text.TfidfVectorizer'>\n",
            "type of tfidf_tran : <class 'scipy.sparse.csr.csr_matrix'>\n"
          ]
        }
      ],
      "source": [
        "print(f'type of tfidf : {type(tfidf)}')\n",
        "print(f'type of tfidf_tran : {type(tfidf_tran)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above cells has created TF-IDF weight of the whole dataset, Now we have to create a vector for the input query in order to calculate inner product of input query and each doc to determine similarty of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Query processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we shoud define a function that applying data_preprocessing funtion on the input query and then create vector of shape (n_docs, n_vocabulary) for it. Then find similarity of input query vector and each document vector with cosine_similarity function ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function returns top 10 results ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def find_similarity(query):\n",
        "    #apply preprocessing on the input query\n",
        "    clean_query = data_preprocessing(query, stopwords)\n",
        "    # create query vector using tfidf model\n",
        "    query_vec = tfidf.transform([clean_query]) # Ip -- (n_docs,x), Op -- (n_docs,n_Feats) \n",
        "    # calculate inner product of each document vector and query vector\n",
        "    # tfidf_tran : matrix of documents -- shape (n_docs, n_vocabulary=n_feats)\n",
        "    results = cosine_similarity(tfidf_tran, query_vec).reshape((-1,)) # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
        "\n",
        "    results_lst = list(results)# use for show the score of each doc\n",
        "    results_lst.sort() \n",
        "\n",
        "    # create dataframe for our output\n",
        "    result_df = pd.DataFrame()\n",
        "    # sort results array then use 10 elements of it from the end of the list then reverse it to have increasing order\n",
        "    # np.argsort :  Returns the indices that would sort an array\n",
        "    # we can change 10 to any number we want \n",
        "    out = results.argsort()[-10:][::-1]\n",
        "    \n",
        "    # add columns to the dataframe\n",
        "    # index: index of a document\n",
        "    for i,index in enumerate(out):\n",
        "        result_df.loc[i,'index'] = str(index)\n",
        "        result_df.loc[i,'title'] = df['title'][index]\n",
        "        result_df.loc[i, 'text'] = df['text'][index]\n",
        "    for j,simScore in enumerate(results_lst[-10:][::-1]):\n",
        "        result_df.loc[j,'Score'] = simScore\n",
        "\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>366</td>\n",
              "      <td>كركس سياه اصفهان از خانه همسايه وارد مي شد</td>\n",
              "      <td>گروه حوادث- شكارچي زنان خانه دار اصفهاني كه با...</td>\n",
              "      <td>0.373880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>226</td>\n",
              "      <td>محققان مي گويند</td>\n",
              "      <td>خانه اي كه كنار خيابان پر رفت و آمد شهر ساخته ...</td>\n",
              "      <td>0.217540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>27</td>\n",
              "      <td>اعتبار بيماران كليويبه وزارت رفاه منتقل مي شود</td>\n",
              "      <td>فارس: معاون سلامت وزارت بهداشت از انتقال اعتبا...</td>\n",
              "      <td>0.187749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>362</td>\n",
              "      <td>خلافكار حرفه اي، باجه فروش مواد راه انداخته بود</td>\n",
              "      <td>گروه حوادث- با دستگيري يك فروشنده مواد مخدر كه...</td>\n",
              "      <td>0.159569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>588</td>\n",
              "      <td>خبر</td>\n",
              "      <td>اختصاص اعتباري ويژه براي كارآفريني در بودجه 86...</td>\n",
              "      <td>0.147407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>612</td>\n",
              "      <td>دريچه بردن كار از اداره به خانه</td>\n",
              "      <td>آرش سيواني هوا كم كم داشت تاريك مي شد، در ادار...</td>\n",
              "      <td>0.146634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>137</td>\n",
              "      <td>دستگيري سارق در هتل مجلل</td>\n",
              "      <td>گروه حوادث- سارق 400ميليون توماني براي آنكه مخ...</td>\n",
              "      <td>0.145817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>124</td>\n",
              "      <td>مهلت 2 ماهه وزارت رفاه براي راه اندازي بانك ا...</td>\n",
              "      <td>فارس: عضو كميسيون اجتماعي مجلس از مهلت 2 ماهه ...</td>\n",
              "      <td>0.143144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>243</td>\n",
              "      <td>اعتراف سارق پر رو در دادسراي شميرانات اموال م...</td>\n",
              "      <td>گروه حوادث- دزد شب رو كه پس از دستبرد به خانه ...</td>\n",
              "      <td>0.140551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>136</td>\n",
              "      <td>راز تغيير چهره جنايتكار 3 ميليارد توماني</td>\n",
              "      <td>گروه حوادث- پرونده جنايتكاري كه بعد از قتل يك ...</td>\n",
              "      <td>0.136532</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  index                                              title  \\\n",
              "0   366        كركس سياه اصفهان از خانه همسايه وارد مي شد    \n",
              "1   226                                   محققان مي گويند    \n",
              "2    27    اعتبار بيماران كليويبه وزارت رفاه منتقل مي شود    \n",
              "3   362   خلافكار حرفه اي، باجه فروش مواد راه انداخته بود    \n",
              "4   588                                               خبر    \n",
              "5   612                   دريچه بردن كار از اداره به خانه    \n",
              "6   137                          دستگيري سارق در هتل مجلل    \n",
              "7   124   مهلت 2 ماهه وزارت رفاه براي راه اندازي بانك ا...   \n",
              "8   243   اعتراف سارق پر رو در دادسراي شميرانات اموال م...   \n",
              "9   136          راز تغيير چهره جنايتكار 3 ميليارد توماني    \n",
              "\n",
              "                                                text     Score  \n",
              "0  گروه حوادث- شكارچي زنان خانه دار اصفهاني كه با...  0.373880  \n",
              "1  خانه اي كه كنار خيابان پر رفت و آمد شهر ساخته ...  0.217540  \n",
              "2  فارس: معاون سلامت وزارت بهداشت از انتقال اعتبا...  0.187749  \n",
              "3  گروه حوادث- با دستگيري يك فروشنده مواد مخدر كه...  0.159569  \n",
              "4  اختصاص اعتباري ويژه براي كارآفريني در بودجه 86...  0.147407  \n",
              "5  آرش سيواني هوا كم كم داشت تاريك مي شد، در ادار...  0.146634  \n",
              "6  گروه حوادث- سارق 400ميليون توماني براي آنكه مخ...  0.145817  \n",
              "7  فارس: عضو كميسيون اجتماعي مجلس از مهلت 2 ماهه ...  0.143144  \n",
              "8  گروه حوادث- دزد شب رو كه پس از دستبرد به خانه ...  0.140551  \n",
              "9  گروه حوادث- پرونده جنايتكاري كه بعد از قتل يك ...  0.136532  "
            ]
          },
          "execution_count": 224,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_similarity('وزارت خانه')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "search.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "2f94ded12a3d328af6c2a1a321fedfc64cc95c083125642f7fbebb469d1cda77"
    },
    "kernelspec": {
      "display_name": "Python 3.7.0 64-bit ('flask': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
